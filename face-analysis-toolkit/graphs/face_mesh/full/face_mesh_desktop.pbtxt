# MediaPipe Facemesh solution extension that support face aligment, blink detection
# Full with annotated video output

# Path to the input video file. (string)
input_side_packet: "input_video_path"
# Path to the output video file. (string)
input_side_packet: "output_video_path"

# Multi-face proctor result <std::vector<ProctorResult>>
output_stream: "multi_face_std_prctor_results"

# max_queue_size limits the number of packets enqueued on any input stream
# by throttling inputs to the graph. This makes the graph only process one
# frame per time.
max_queue_size: 1

# Decodes an input video file into images and a video header.
node {
  calculator: "OpenCvVideoDecoderCalculator"
  input_side_packet: "INPUT_FILE_PATH:input_video_path"
  output_stream: "VIDEO:input_video"
  output_stream: "VIDEO_PRESTREAM:input_video_header"
}

# Defines side packets for further use in the graph.
node {
  calculator: "ConstantSidePacketCalculator"
  output_side_packet: "PACKET:0:num_faces"
  output_side_packet: "PACKET:1:with_attention"
  node_options: {
    [type.googleapis.com/mediapipe.ConstantSidePacketCalculatorOptions]: {
      packet { int_value: 1 }
      packet { bool_value: true }
    }
  }
}

# Subgraph that detects faces and corresponding landmarks.
node {
  calculator: "FaceLandmarkFrontCpu"
  input_stream: "IMAGE:input_video"
  input_side_packet: "NUM_FACES:num_faces"
  input_side_packet: "WITH_ATTENTION:with_attention"
  output_stream: "LANDMARKS:multi_face_landmarks"
  output_stream: "ROIS_FROM_LANDMARKS:face_rects_from_landmarks"
  output_stream: "DETECTIONS:face_detections"
  output_stream: "ROIS_FROM_DETECTIONS:face_rects_from_detections"
}

# Outputs each element of multi_face_landmarks at a fake timestamp for the rest
# of the graph to process. At the end of the loop, outputs the BATCH_END
# timestamp for downstream calculators to inform them that all elements in the
# vector have been processed.
node {
  calculator: "BeginLoopNormalizedLandmarkListVectorCalculator"
  input_stream: "ITERABLE:multi_face_landmarks"
  output_stream: "ITEM:face_landmarks"
  output_stream: "BATCH_END:landmark_timestamp"
}

# Standardize the landmarks
node {
  calculator: "LandmarkStandardizationCalculator"
  input_stream: "face_landmarks"
  output_stream: "face_std_landmarks"
}

# Detect face movements
node {
  calculator: "FaceMovementCalculator"
  input_stream: "face_landmarks"
  output_stream: "face_movement"
}

# Detect facial activity
node {
  calculator: "FaceActivityCalculator"
  input_stream: "face_std_landmarks"
  output_stream: "face_activity"
}

# Detect orientations
node {
  calculator: "FaceOrientationCalculator"
  input_stream: "face_std_landmarks"
  output_stream: "face_orientations"
}

# Detect Eye blink
node {
  calculator: "EyeBlinkCalculator"
  input_stream: "face_std_landmarks"
  output_stream: "face_blinks"
}

node  {
  calculator: "ProctorResultCalculator"
  input_stream: "ORIENT:face_orientations"
  input_stream: "BLINK:face_blinks"
  input_stream: "ACTIVE:face_activity"
  input_stream: "MOVE:face_movement"
  output_stream: "RESULT:face_proctor_results"
}

# Collects a ProctorResult object for each hand into a vector. Upon receiving the
# BATCH_END timestamp, outputs the vector of ProctorResult at the BATCH_END
# timestamp.
node {
  calculator: "EndLoopProctorResultVectorCalculator"
  input_stream: "ITEM:face_proctor_results"
  input_stream: "BATCH_END:landmark_timestamp"
  output_stream: "ITERABLE:multi_face_std_prctor_results"
}

node {
  calculator: "PacketClonerCalculator"
  input_stream: "TICK:throttled_input_video"
  input_stream: "throttled_input_video"
  output_stream: "output_video"
}

# Subgraph that renders face-landmark annotation onto the input video.
# node {
#   calculator: "FaceRendererCpu"
#   input_stream: "IMAGE:input_video"
#   input_stream: "LANDMARKS:multi_face_landmarks"
#   input_stream: "NORM_RECTS:face_rects_from_landmarks"
#   input_stream: "DETECTIONS:face_detections"
#   output_stream: "IMAGE:output_video"
# }

# Encodes the annotated images into a video file, adopting properties specified
# in the input video header, e.g., video framerate.
node {
  calculator: "OpenCvVideoEncoderCalculator"
  input_stream: "VIDEO:output_video"
  input_stream: "VIDEO_PRESTREAM:input_video_header"
  input_side_packet: "OUTPUT_FILE_PATH:output_video_path"
  node_options: {
    [type.googleapis.com/mediapipe.OpenCvVideoEncoderCalculatorOptions]: {
      codec: "avc1"
      video_format: "mp4"
    }
  }
}
